{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce massive amounts of data everyday. Although supervised learning has been at the core of recent success of deep learning, unsupervised learning has the potential to scale with this ever increasing availability of data as it alleviates the need to carefully hand-craft and annotate training datasets.\n",
    "\n",
    "Clustering is one of the fundamental unsupervised method of knowledge discovery. It's goal is to group similar data points together without supervision or prior knowledge of nature of the clusters. Various aspects of clustering such as distance metrics, feature selection, grouping methods etc. have been extensively studied since the origin of cluster analysis in the 1930s. Because of it's importance in exploratory understanding of data, clustering has always been an active field of research.\n",
    "\n",
    "Galvanized by the widespread success of deep learning in both supervised and unsupervised problems, many of the recent work on clustering has been focused on using deep neural networks- often, this pairing is referred to as *deep clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Clustering Framework\n",
    "\n",
    "Deep clustering algorithms can be broken down to three essential components-neural network, network loss, and clustering loss. \n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "A deep neural network is the central part of deep clustering algorithms. They are employed to learn low dimensional non-linear data representations from the dataset. Most widely used architectures are AutoEncoder based, however generative models like Variational Autoencoders and GANs have also been used in different algorithms. Variations in network architecture like feed forward and convolutional neural networks are also widely reported.\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "The loss function of deep-clustering algorithms are generally a linear combination of unsupervised representation learning loss $L_R$ and a loss function to enhance clustering friendly properties of the data, $L_C$. They are formulated as\n",
    "$$\n",
    "L = \\lambda L_R + (1-\\lambda) L_C \n",
    "$$\n",
    "Here, $\\lambda$ is a hyperparameter between 0 and 1 that controls the balance of impact of the two loss functions.\n",
    "\n",
    "**Network Loss**\n",
    "\n",
    "The neural network loss are usually the reconstruction loss of Autoencoder, variational loss of VAEs, or the adversarial loss of GANs. The netwrok loss is essential for the initialization of the deep neural networks. After a few epochs, the clustering loss is introduced by changing the $\\lambda$ hyperparameter to achieve balance and stability in training.\n",
    "\n",
    "Some algorithms like JULE, DAC and IMSAT discard the network loss altogether in favor of just using clustering loss by setting $\\lambda$ to 0.\n",
    "\n",
    "\n",
    "**Clustering Loss**\n",
    "\n",
    "Several different clustering loss has been proposed and used in different algorithms. There are generally two kinds of clustering loss:\n",
    "    \n",
    "1. Cluster Assignment\n",
    "    \n",
    "    Cluster assignment losses provides cluster assignments to the data points directly, and no further clustering is required on the learnt representations. Some examples are: k-means loss (DCN), cluster assignment hardening loss (DEC), agglomerative clustering loss (JULE) etc.\n",
    "    \n",
    "2. Cluster Regularization\n",
    "    \n",
    "    Cluster regularization loss, on the other hand, only enforces the network to preserve suitable discriminant information from the data in the representations. Further clustering on the representation space is necessary to obtain the clustering result. Some examples are: locality preserving loss, group sparsity loss (DEN) etc.\n",
    "    \n",
    "### Performance Metrics\n",
    "\n",
    "In deep-clustering literature, we see the regular use of the following three evaluation metrics:\n",
    "\n",
    "1. Unsupervised Clustering Accuracy (ACC)\n",
    "\n",
    "    ACC is the unsupervised equivalent of classification accuracy. ACC differs from the usual accuracy metric such that it uses a mapping function $m$ to find the best mapping between the cluster assignment output $c$ of the algorithm with the ground truth $yâ€‹$.  This mapping is required because an unsupervised algorithm may use a different label than the actual ground truth label to represent the same cluster. A reference python implementation can be found [here](https://github.com/XifengGuo/DEC-keras/blob/master/metrics.py).\n",
    "$$\n",
    "    ACC = max_{m} \\frac{\\sum_{i=1}^n1\\{y_i = m(c_i)\\}}{n}\n",
    "$$\n",
    "    \n",
    "2. Normalized Mutual Information (NMI)\n",
    "\n",
    "    NMI is an information theoretic metric that measures the mutual information between the cluster assignments and the ground truth labels. It is normalized by the average of entropy of both ground labels and the cluster assignments. Sklearn's implementation is available as `sklearn.metrics.normalized_mutual_info_score`.\n",
    "$$\n",
    "NMI(Y,C) = \\frac{I(Y,C)}{\\frac{1}{2}[H(Y)+H(C)]}\n",
    "$$\n",
    "\n",
    "3. Adjusted Rand Index (ARI)\n",
    "\n",
    "    The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. Sklearn's implementation is available as `sklearn.metrics.adjusted_rand_score`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Approaches on Deep Clustering\n",
    "\n",
    "Based on the different network architectures and the nature of loss functions used, we can broadly categorize current deep learning models into following three categories:\n",
    "\n",
    "- Survey paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8412085\n",
    "\n",
    "### AutoEncoders based\n",
    "\n",
    "Autoencoders have found extensive use in unsupervised representation learning tasks ranging from denoising to neural machine translation. The simple yet very powerful framework of learning representations from data by encoding and decoding is also widely used to learn cluster friendly representations.\n",
    "\n",
    "Most of the AE based deep-clustering approaches use a pre-training scheme in which the encoder and decoder network parameters are initialized with the reconstruction loss before clustering loss is introduced.\n",
    "\n",
    "**Deep Embedded Clustering (DEC)** [[paper](https://arxiv.org/pdf/1511.06335.pdf)] [[code](https://github.com/XifengGuo/DEC-keras)]\n",
    "\n",
    "Deep Embedded Clustering is a pioneering work on deep-clustering, and is often used as the benchmark for comparing performance of other models. DEC uses AE reconstruction loss and cluster assignment hardeining loss. It defines soft cluster assignment distribution $q$ based on Student's t-distribution with degree of freedom $\\alpha$ set to 1. To further refine the assignments, it also defines an auxiliary target distribution derived from this assignment $p_{ij}$, which is updated after every $T$ iterations.\n",
    "$$\n",
    "q_{i j}=\\frac{\\left(1+\\left\\|z_{i}-\\mu_{j}\\right\\|^{2} / \\alpha\\right)^{-\\frac{\\alpha+1}{2}}}{\\sum_{j^{\\prime}}\\left(1+\\left\\|z_{i}-\\mu_{j^{\\prime}}\\right\\|^{2} / \\alpha\\right)^{-\\frac{\\alpha+1}{2}}}\n",
    "$$\n",
    "$$\n",
    "p_{i j}=\\frac{q_{i j}^{2} / f_{j}}{\\sum_{j^{\\prime}} q_{i j^{\\prime}}^{2} / f_{j^{\\prime}}}\n",
    "$$\n",
    "\n",
    "The training begins with a pre-training stage to initialize encoder and decoder parameters for a few epochs with reconstruction loss. After pre-training, it removes the decoder network and the encoder network is then fine-tuned by optimizing KL divergence between soft cluster assignment $q_{ij}$ and auxilliary distribution $p_{ij}$. This training can be thought of as a self-training process to refine the representations while doing cluster assignment iteratively.\n",
    "  \n",
    "$$\n",
    "\\min \\sum_{i} \\sum_{j} p_{i j} \\log \\frac{p_{i j}}{q_{i j}}\n",
    "$$\n",
    "  \n",
    "*Clustering Accuracy (ACC): MNIST 86.5%, USPS 74.08%, Reuters 73.68%*\n",
    "\n",
    "**Discriminately Boosted Clustering (DBC)** [[paper](https://arxiv.org/pdf/1703.07980.pdf)]\n",
    "\n",
    "Discriminately Boosted Clustering builds on DEC by using convolutional autoencoder instead of feed forward autoencoder. It uses the same training scheme, reconstruction loss and *cluster assignment hardening loss* as DEC. DBC achieves very good results on image datasets because of it's use of convolutional neural network.\n",
    "  \n",
    "    \n",
    "**Deep Clustering Network (DCN)** [[paper](https://arxiv.org/pdf/1610.04794v1.pdf)] [[code](https://github.com/boyangumn/DCN-New)]\n",
    "\n",
    "Deep Clustering Network combines an autoencoder with K-means algorithm to learn representations that are amenable to K-means. It pre-trains the autoencoder, and then jointly optimizes the reconstruction loss and K-means loss with alternating cluster assignments. The k-means clustering loss is very intuitive and simple compared to other methods. DCN defines it's objective as:\n",
    "    \n",
    "$$\n",
    "\\min \\sum_{i=1}^{N}\\left(\\ell\\left(\\boldsymbol{g}\\left(\\boldsymbol{f}\\left(\\boldsymbol{x}_{i}\\right)\\right), \\boldsymbol{x}_{i}\\right)+\\frac{\\lambda}{2}\\left\\|\\boldsymbol{f}\\left(\\boldsymbol{x}_{i}\\right)-\\boldsymbol{M} \\boldsymbol{s}_{i}\\right\\|_{2}^{2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{f}$ and $\\boldsymbol{g}$ are encoder and decoder functions respectively, $s_i$ is the assignment vector of data point $i$ which has only one non-zero element and $M_k$, denotes the centroid of the $k$th cluster.\n",
    "\n",
    "*Clustering Accuracy (ACC): MNIST 93%, Pendigits 72%, 20NewsGroup 44%*\n",
    "\n",
    "**Deep Embedded Regularized Clustering (DEPICT)** [[paper](https://arxiv.org/pdf/1704.06327.pdf)] [[code](https://github.com/herandy/DEPICT)]\n",
    "\n",
    "Deep Embedded Regularized Clustering consists of several tricks. It uses softmax layer stacked on top of convolutional autoencoder with a noisy encoder. It jointly optimizes reconstruction loss and cross entropy loss of softmax assignments and it's auxilliary assignments which leads to balanced cluster assignment loss. All the layers of the encoder and decoder also contribute to the reconstruction loss instead of just input and output layers.\n",
    "    \n",
    "$$\n",
    "p_{i k}=\\frac{\\exp \\left(\\boldsymbol{\\theta}_{k}^{T} \\mathbf{z}_{i}\\right)}{\\sum_{k^{\\prime}=1}^{K} \\exp \\left(\\boldsymbol{\\theta}_{k^{\\prime}}^{T} \\mathbf{z}_{i}\\right)}\n",
    "$$\n",
    "$$\n",
    "q_{i k}=\\frac{p_{i k} /\\left(\\sum_{i^{\\prime}} p_{i^{\\prime} k}\\right)^{\\frac{1}{2}}}{\\sum_{k^{\\prime}} p_{i k^{\\prime}} /\\left(\\sum_{i^{\\prime}} p_{i^{\\prime} k^{\\prime}}\\right)^{\\frac{1}{2}}}\n",
    "$$\n",
    "$$\n",
    "\\min _{\\boldsymbol{\\psi}}-\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} q_{i k} \\log p_{i k}\n",
    "$$\n",
    "\n",
    "DEPICT achieves very impressive clustering performance as a result of these improvements.\n",
    "\n",
    "*Clustering Accuracy (ACC): MNIST 96.5%, USPS 96.4%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generative Model Based\n",
    "\n",
    "Generative models like Variational Autoencoders and Generative Adversarial Networks learn latent representation space that can be interpolated to generate new samples from the data distribution.\n",
    "\n",
    "**Variational Deep Embedding (VaDE)** [[paper](https://arxiv.org/pdf/1611.05148.pdf)] [[code](https://github.com/slim1017/VaDE)]\n",
    "\n",
    "VaDE incorporates probabilistic clustering problem within the framework of VAE by imposing a GMM priori over VAE. The optimization essentially minimizes reconstruction loss and KL divergence between Mixture of Gaussians prior $c$ to the variational posterior to learn a uniform latent space with clusters which allows interpolation to generate new samples. \n",
    "    \n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{ELBO}}(\\mathbf{x})=E_{q(\\mathbf{z}, c | \\mathbf{x})}[\\log p(\\mathbf{x} | \\mathbf{z})]-D_{K L}(q(\\mathbf{z}, c | \\mathbf{x}) \\| p(\\mathbf{z}, c))\n",
    "$$\n",
    "\n",
    "After the optimization, the cluster assignments can be inferred directly from the MoG prior. One strong advantage of VaDE is that it stands on the strong theoretical ground of VAE.\n",
    "\n",
    "*Clustering Accuracy (ACC): MNIST 94.46%, Reuters 79.38%, STL-10 84.45%*\n",
    "\n",
    "**Information Maximizing Generative Adversarial Network (InfoGAN)** [[paper](https://arxiv.org/pdf/1606.03657.pdf)] [[code](https://github.com/openai/InfoGAN)]\n",
    "    \n",
    "InfoGAN is another generative approach under GAN framework whose primary objective is to learn disentangled representations. InfoGAN decomposes the input into two parts: incompressible noise $z$ and latent code $c$, so the form of the generator becomes $G(z, c)$. It then combines standard GAN objective with information-theoretic regularization $I(c; G(z, c))$. When choosing to model the latent codes with one categorical code having k values, and several continuous codes, it has the function of clustering data points into k clusters. \n",
    "\n",
    "$$\n",
    "\\min _{G} \\max _{D} V_{I}(D, G)=V(D, G)-\\lambda I(c ; G(z, c))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Cluster Optimization\n",
    "\n",
    "The third category of deep clustering models discard any reconstruction loss and use clustering loss directly to optimize the deep neural network.\n",
    "\n",
    "**Joint Unsupervised Learning (JULE)** [[paper](https://arxiv.org/pdf/1604.03628.pdf)] [[code](https://github.com/FJR-Nancy/joint-cluster-cnn)]\n",
    "\n",
    "JULE  uses a convolutional neural network with agglomerative clustering loss without any reconstruction loss.  In every iteration, hierachical clustering is performed on the forward pass using affinity measure $\\boldsymbol{\\mathcal{A}}$ and representations are optimized on the backward pass. JULE reports excellent performance on image datasets. However it has one significant limitation-agglomerative clustering requires the construction of undirected affinity matrix which causes JULE to suffer from computational and memory complexity issues.\n",
    "\n",
    "$$\n",
    "\\min -\\frac{\\lambda}{K_{c}-1} \\sum_{i, j, k}\\left(\\gamma \\mathcal{A}\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)-\\mathcal{A}\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{k}\\right)\\right)\n",
    "$$\n",
    "\n",
    "*Clustering Accuracy (ACC): MNIST 97.3%, CMU-PIE 100%, USPS 95.5%*\n",
    "\n",
    "**Deep Adaptive Image Clustering (DAC)** [[paper](http://openaccess.thecvf.com/content_ICCV_2017/papers/Chang_Deep_Adaptive_Image_ICCV_2017_paper.pdf)] [[code](https://github.com/vector-1127/DAC)]\n",
    "\n",
    "DAC uses convolutional neural network with a binary pairwise classification as clustering loss. The method is motivated from a basic assumption that the relationship between pair-wise images is binary i.e. $r_{ij}$ = 1 indicates that $x_i$ and $x_j$ belong to the same cluster and $r_{ij}$ = 0 otherwise. It also adds a regularization constraint that helps learn label features as one hot encoded features, and the similarity $g(x_i,x_j)$ is computed as the dot product of these label features. DAC also reports superior perfromance on benchmark datasets.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{L\\left(r_{i j}, g\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j} ; \\mathbf{w}\\right)\\right)=} {-r_{i j} \\log \\left(g\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j} ; \\mathbf{w}\\right)\\right)-\\left(1-r_{i j}\\right) \\log \\left(1-g\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j} ; \\mathbf{w}\\right)\\right)}\\end{array}\n",
    "$$\n",
    "\n",
    "*Clustering Accuracy (ACC): MNIST 97.75%, STL-10 46.9%, ImageNet-10 52.7%*\n",
    "\n",
    "\n",
    "**Information Maximizing Self-Augmented Training (IMSAT)** [[paper](https://arxiv.org/pdf/1702.08720.pdf)] [[code](https://github.com/shenyuanyuan/IMSAT)]\n",
    "\n",
    "IMSAT learns discrete representations of data using information maximization between input and cluster assignment. It proposes Self Augmentation Training, which penalizes representation dissimilarity between the original data points and augmented ones $T(x)$.\n",
    "\n",
    "$$\n",
    "{\\mathcal{R}_{\\mathrm{SAT}}(\\theta ; x, T(x))} {\\quad=-\\sum_{m=1}^{M} \\sum_{y_{m}=0}^{V_{m}-1} p_{\\widehat{\\theta}}\\left(y_{m} | x\\right) \\log p_{\\theta}\\left(y_{m} | T(x)\\right)}\n",
    "$$\n",
    "It combines mutual information constraint along with SAT scheme to define objective function as:\n",
    "\n",
    "$$\n",
    "\\min \\mathcal{R}_{\\mathrm{SAT}}(\\theta ; T)-\\lambda[H(Y)-H(Y | X)]\n",
    "$$\n",
    "\n",
    "*Clustering Accuracy (ACC): MNIST 98.4%, 20NewsGroup 31.1%, Reuters 71.9%*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "1. Hyperparameters\n",
    "\n",
    "2. Lack of theoritical framework\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
