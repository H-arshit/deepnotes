---
layout: post
title: Introduction to Convolutional Neural Networks
---
Using fully connected layers to classify images is strange. It treats the input pixels which are far apart and close together on exactly on the same footing. What if we used an architecture that tries to take the advantage of the spatial structure of the image?

Convolutional neural networks are a variant of regular neural network that take this advantage of the spatial nature of the image by making explicit assumption that the input are images. Instead of a linear list of neurons that make up a layer in regular neural network, they have a series of layers that take 3D input volume, perform a differentiable function with or without learnable parameters and transform the input to 3D output volume.

Convolutional neural network benefit from these basic but powerful concepts:

### Local Receptive Fields

Unlike in a fully connected neural network, ConvNets don't have every neuron in one layer connected to every neuron in the next layer.  Instead, we only make connections in small localized regions of the input image. This region in input image is called the *local receptive field.* 

We then slide the local receptive field across the entire input image. For each field, there is a different hidden neuron in its first hidden layer. We can pad the input image around the border by introducing a hyperparameter called *zero padding*, which makes computation convenient and prevents fast loss of data at border. 

The sliding of the local receptive field can be controlled by another hyperparameter called *stride length*, which is the number of pixels the field is moved at a time. Both these hyperparameters control the spatial size of the output volume.

### Shared Weights and Biases

Another peculiarity of ConvNets is that we use the same weights and biases for each of the hidden neurons. This means that all the neurons in the layer detect exactly the same feature, just at different locations in the input. By sharing the weights and biases, the network is compelled to learn the weights to detect a feature at different parts of the image. Suppose in a layer the network learns the weights to detect a vertical edge in a local receptive field. This ability is likely to be useful at other places in the image too.

This makes ConvNets well adapted to translation invariance of image, which means the network tries to detect a feature in the image, and once it detects the feature, the location of the feature becomes irrelevant. 

These weights that define the feature map are also called kernel or filter. To perform image recognition, we need more than one feature map. Thus a convolutional layer consists of several different feature maps.

A big advantage of sharing weights and biases is that it also greatly reduces number of parameters that the network needs to learn. Regular fully connected neural networks tend to use generally 40 times more parameters than ConvNets. This allows us to train faster and help build deep networks. Less number of parameters also means less chance of over fitting.

### Pooling Layers

Pooling layers are another layer in ConvNets used after convolutional layers. What these layers do is that they simplify or summarize the information from the convolution layer by performing a statistical aggregate like average or max. The most common form of pooling layer used is *maxpooling*, which simply outputs the maximum activation of a certain region of the convolution layer's output. Another common approach known as *L2 pooling* which takes the square root of the sum of the squares of the activations. So pooling layers essentially take each feature maps and produce a down sampled feature maps. This means the number of parameters in the network is greatly reduced while preserving the feature information.

### Using Rectified Linear Units

ConvNets mostly use rectified linear units or ReLU as activation function. It clamps the negative activation to 0 and is defined as $$f(z) = max(0,Z)$$. Other activations like *tanh* and *sigmoid* suffer from learning slowdown when they saturate, but ReLU never saturates, so there is no learning slowdown. ReLU units have shown to outperform networks based on sigmoid and tanh activation functions and are generally implemented as an explicit layer in ConvNets which performs the element wise activation function to the input. 

![ A plot from Krizhevsky et al.](/public/images/alexplot.jpeg)

 A plot from [Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit.

### Putting it all together

ConvNet takes the advantage of spatial information of the input. The Convolutional and Pooling layers learn about local spatial structure in the input image, while the latter fully connected layer learns at more abstract level, integrating the knowledge from these other layers to perform classification. 

The architecture is laid out as a series of layers with a few pair of Convolution and ReLU layers and a pooling layer once in a while to down sample the input volume. Finally we have fully connected layer/s which produce output scores. 

All the layers implement a forward and backward API. Forward propagation calculates the activations, and backward propagation uses the gradient from above and the local gradient to calculate gradients for the layer's learnable parameters, and the input gradient which is propagated back to the earlier layer.

### What about the vanishing gradient problem?

We know that in deep neural networks, gradients are unstable, leading to either vanishing or exploding in earlier layers. How does ConvNet overcome this problem?

The thing is we haven't avoided the problem altogether, but taken a few approaches to minimize it:

1. Convolution layers and pooling layers greatly reduce the number of parameters, making learning problem much easier
2. Using more powerful regularization methods like convolution layer and possibly dropout and batch normalization to reduce overfitting.
3. Using ReLU non linearity to speed up training
4. Other ideas like using cross entropy cost function and good weight initialization
